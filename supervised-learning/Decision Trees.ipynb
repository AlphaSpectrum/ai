{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy). Leaf node (e.g., Play) represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.\n",
    "\n",
    "![](../assets/Decision_Tree_1.png)\n",
    "\n",
    "## Entropy\n",
    "A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous). ID3 algorithm uses entropy to calculate the homogeneity of a sample. If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one.\n",
    "\n",
    "$$\n",
    "entropy = -p_1log_2(p_1)-p_2log_2(p_2)-\\dots-p_nlog_2(p_n) = \\sum^{n}_{n=1}p_ilog_2(p_i)\n",
    "$$\n",
    "where $p_1=\\frac{m}{m+n}, p_2=\\frac{m}{m+n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** If we have a bucket with *eight* red balls, *three* blue balls, and *two* yellow balls, what is the entropy of the set of balls?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "|Color|Quantity|Probability|\n",
    "|--|--|--|\n",
    "|red|8|$\\frac{8}{13}$|\n",
    "|blue|3|$\\frac{3}{13}$|\n",
    "|yellow |2|$\\frac{2}{13}$|\n",
    "\n",
    "$$\n",
    "E=-\\frac{8}{13}log_2(\\frac{8}{13})-\\frac{3}{13}log_2(\\frac{3}{13})-\\frac{2}{13}log_2(\\frac{2}{13})=0.473\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "\n",
    "The entropy (very common in Information Theory) characterizes the (im)purity of an arbitrary collection of examples.\n",
    "\n",
    "Information Gain is the expected reduction in entropy caused by partitioning the examples according to a given attribute.\n",
    "\n",
    "With entropy defined as: \n",
    "\n",
    "$$\n",
    "E=-\\sum^{n}_{i=1}p_nlog_2p_n\n",
    "$$\n",
    "\n",
    "Then the change in entropy, or Information Gain, is defined as:\n",
    "\n",
    "$$\n",
    "\\Delta{H}=H_{parent}-\\frac{m}{m+n}H_{child^{(1)}}-\\frac{n}{m+n}H_{child^{(2)}}\n",
    "$$\n",
    "\n",
    "where $m$ is the total number of instances, with $m_k$ instances\n",
    "belonging to class $k$, where $k=1,\\dots,n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Which of the following splitting criteria provides the most information gain for discriminating Mobugs from Lobugs?\n",
    "\n",
    "|Species|Color|Length (mm)|\n",
    "|-------|-----|-----------|\n",
    "|Mobug  |Brown|11.6       |\n",
    "|Mobug  |Blue |16.3       |\n",
    "|Lobug  |Blue |15.1       |\n",
    "|Lobug  |Green|23.7       |\n",
    "|Lobug  |Blue |18.4       |\n",
    "|Lobug  |Brown|17.1       |\n",
    "|Mobug  |Brown|15.7       |\n",
    "|Lobug  |Green|18.6       |\n",
    "|Lobug  |Blue |22.9       |\n",
    "|Lobug  |Blue |21.0       |\n",
    "|Lobug  |Blue |20.5       |\n",
    "|Mobug  |Green|21.2       |\n",
    "|Mobug  |Brown|13.8       |\n",
    "|Lobug  |Blue |14.5       |\n",
    "|Lobug  |Green|24.8       |\n",
    "|Mobug  |Brown|18.2       |\n",
    "|Lobug  |Green|17.9       |\n",
    "|Lobug  |Green|22.7       |\n",
    "|Mobug  |Green|19.9       |\n",
    "|Mobug  |Blue |14.6       |\n",
    "|Mobug  |Blue |19.2       |\n",
    "|Lobug  |Brown|14.1       |\n",
    "|Lobug  |Green|18.8       |\n",
    "|Mobug  |Blue |13.1       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9182958340544896\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def two_group_ent(first, tot):\n",
    "    return -(first/tot*np.log2(first/tot) + (tot-first)/tot*np.log2((tot-first)/tot))\n",
    "\n",
    "tot_ent = two_group_ent(10, 24)\n",
    "g17_ent = 15/24 * two_group_ent(11,15) + 9/24 * two_group_ent(6,9)\n",
    "answer = tot_ent - g17_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}